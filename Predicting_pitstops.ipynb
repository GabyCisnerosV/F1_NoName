{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the number of pit stops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of pit stops and their duration have decreased with the years. In the first phase of this subproject I'll try to predict how many stops will there be in the race and later I would like to predict when yould it happen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bur first, some EDA..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image \n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"https://media.giphy.com/media/MovqJSMROh1gA/giphy.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the main dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The data stored in this path is obtained from the API of https://ergast.com/mrd/. It is continuously updated.\n",
    "#To update this data please run the file \"API_Requests_Results_Qualifying_Laps_PitStops.py\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "path = 'C:/Users/gabri/Dropbox/Gaby/Proyectos/My_Portafolio/F1/Data/'\n",
    "\n",
    "PitsDF=pd.read_csv(path+\"PitsDF.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should import info from the results data so that we can know the result of each driver at the end of each race. We should only take into account the drivers that ended each race. If the driver do not ends the race, they will probably have no pit stops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ResultsDF=pd.read_csv(path+\"ResultsDF.csv\")\n",
    "PitsDF=PitsDF.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforming pits duration into seconds:\n",
    "PitsDF[['durationSEC','nothing']]=PitsDF['duration'].str.split(\":\", expand=True)\n",
    "PitsDF['duration_in_sec']=PitsDF['durationSEC'].astype(float)\n",
    "PitsDF=PitsDF.drop(columns=['durationSEC','nothing'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Season-Round feature:\n",
    "ResultsDF[\"Season-Round-Driver\"]=ResultsDF[\"season\"].astype(str)+\"-\"+ResultsDF[\"round\"].astype(str)+\"-\"+ResultsDF[\"Driver.driverId\"].astype(str)\n",
    "PitsDF[\"Season-Round-Driver\"]=PitsDF[\"season\"].astype(str)+\"-\"+PitsDF[\"round\"].astype(str)+\"-\"+PitsDF[\"driverId\"].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Left Join of the Pits DF with the Results DF\n",
    "PitsExtraDF=PitsDF.merge(ResultsDF[[\"Season-Round-Driver\",\"status\",'Constructor.constructorId','Constructor.name',\"laps\"]],on=\"Season-Round-Driver\",how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top number of pits per race:\n",
    "PitsExtraDF[\"stop\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dividing the main df into separate ones taking into account the number of stops, to then unify them:\n",
    "Pits1ExtraDF = PitsExtraDF[PitsExtraDF[\"stop\"]==1].rename(columns={'lap': 'Pit1_lap',\"time\":\"Pit1_time\",\"duration_in_sec\":\"Pit1_duration\"})\n",
    "Pits2ExtraDF = PitsExtraDF[PitsExtraDF[\"stop\"]==2].rename(columns={'lap': 'Pit2_lap',\"time\":\"Pit2_time\",\"duration_in_sec\":\"Pit2_duration\"})\n",
    "Pits3ExtraDF = PitsExtraDF[PitsExtraDF[\"stop\"]==3].rename(columns={'lap': 'Pit3_lap',\"time\":\"Pit3_time\",\"duration_in_sec\":\"Pit3_duration\"})\n",
    "Pits4ExtraDF = PitsExtraDF[PitsExtraDF[\"stop\"]==4].rename(columns={'lap': 'Pit4_lap',\"time\":\"Pit4_time\",\"duration_in_sec\":\"Pit4_duration\"})\n",
    "\n",
    "PitsUnified=Pits1ExtraDF[['Season-Round-Driver', 'date', 'status', 'driverId', 'season','round', 'raceName', 'Circuit.circuitId', 'Circuit.circuitName',\n",
    "       'Circuit.Location.country','Constructor.constructorId', 'Constructor.name', 'laps',\n",
    "       'Pit1_lap', 'Pit1_time', 'Pit1_duration']]\n",
    "\n",
    "#Unifying the separate datasets forming one dataset with one row per race and per driver\n",
    "PitsUnified=PitsUnified.merge(Pits2ExtraDF[[\"Season-Round-Driver\",'Pit2_lap', 'Pit2_time', 'Pit2_duration']],on=\"Season-Round-Driver\",how=\"left\")\n",
    "PitsUnified=PitsUnified.merge(Pits3ExtraDF[[\"Season-Round-Driver\",'Pit3_lap', 'Pit3_time', 'Pit3_duration']],on=\"Season-Round-Driver\",how=\"left\")\n",
    "PitsUnified=PitsUnified.merge(Pits4ExtraDF[[\"Season-Round-Driver\",'Pit4_lap', 'Pit4_time', 'Pit4_duration']],on=\"Season-Round-Driver\",how=\"left\")\n",
    "\n",
    "#Replacing na values of pits columns with 0\n",
    "PitsUnified=PitsUnified.fillna(0)\n",
    "\n",
    "len(PitsUnified) #counting the number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding the total number of pits per driver and per race\n",
    "conditions = [\n",
    "    (PitsUnified['Pit4_lap'] > 0),\n",
    "    (PitsUnified['Pit4_lap'] == 0) & (PitsUnified['Pit3_lap'] > 0),\n",
    "    (PitsUnified['Pit4_lap'] == 0) & (PitsUnified['Pit3_lap'] == 0) & (PitsUnified['Pit2_lap'] > 0),\n",
    "    (PitsUnified['Pit4_lap'] == 0) & (PitsUnified['Pit3_lap'] == 0) & (PitsUnified['Pit2_lap'] == 0) & (PitsUnified['Pit1_lap'] > 0)\n",
    "]\n",
    "\n",
    "values = [4, 3, 2, 1]\n",
    "\n",
    "PitsUnified['Num_Pits'] = np.select(conditions, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the feature of laps per pitstop. This tells us how many laps in average you can do between pit stops\n",
    "PitsUnified[\"LapsbetweenPitstops\"]=PitsUnified[\"laps\"]/PitsUnified['Num_Pits']\n",
    "\n",
    "#Pits of drivers that ended the races:\n",
    "PitsUnified_Finished=PitsUnified[PitsUnified[\"status\"]==\"Finished\"].reset_index()\n",
    "\n",
    "#Now we have one row per race and driver, only of the drivers who finished the race\n",
    "PitsUnified_Finished.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average numper of pit stops per circuit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mugello circuit and Park Zandvoort are on average the circuits that have the highest number of pit stops per race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"darkgrid\", palette=\"magma\",font_scale=1.2,font=\"serif\")\n",
    "sns.catplot(data=PitsUnified_Finished,x=\"Circuit.circuitName\", y=\"Num_Pits\",kind=\"bar\",height=12,aspect=2)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average numper of pit stops per year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of pit stops on average decreased in 2018. This might have been caused by several causes like changes in the regulations, different circuits in that season, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"darkgrid\", palette=\"magma\",font_scale=1.2,font=\"serif\")\n",
    "sns.catplot(data=PitsUnified_Finished,x=\"season\", y=\"Num_Pits\",kind=\"bar\",height=4,aspect=2)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average number of laps between pit stops per circuit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that contrary to what it was believed, the number of laps on average per pit stop have increased and decreased depending on each circuit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting specific circuits where at least two seasons a race has been helds=\n",
    "SelectedCircuits=['albert_park', 'sepang', 'shanghai', 'bahrain', 'catalunya',\n",
    "       'monaco', 'villeneuve', 'silverstone',\n",
    "       'hockenheimring', 'hungaroring', 'spa', 'monza', 'marina_bay',\n",
    "       'suzuka', 'yas_marina', 'americas',\n",
    "       'interlagos', 'nurburgring', 'red_bull_ring', 'sochi', 'rodriguez',\n",
    "       'BAK']\n",
    "\n",
    "PitsUnified_Finished_Selected=PitsUnified_Finished[PitsUnified_Finished[\"Circuit.circuitId\"].isin(SelectedCircuits) == True]\n",
    "\n",
    "sns.set_theme(style=\"darkgrid\", palette=\"magma\",font_scale=0.8,font=\"serif\")\n",
    "sns.relplot(data=PitsUnified_Finished_Selected,x=\"season\", y=\"LapsbetweenPitstops\",col=\"Circuit.circuitName\",col_wrap=4,kind=\"line\",height=2,aspect=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average number of laps between pit stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"darkgrid\", palette=\"magma\",font_scale=1.2,font=\"serif\")\n",
    "sns.catplot(data=PitsUnified_Finished,x=\"season\", y=\"LapsbetweenPitstops\",kind=\"bar\",height=4,aspect=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#Features to encode:\n",
    "to_encode=['Season-Round-Driver', 'driverId', 'Circuit.circuitId','Circuit.Location.country', 'Constructor.constructorId']\n",
    "\n",
    "for n in to_encode:\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(PitsUnified_Finished[n])\n",
    "    nameencoded=n+\"_enc\"\n",
    "    encoders=encoder.transform(PitsUnified_Finished[n])\n",
    "    PitsUnified_Finished[nameencoded]=encoders\n",
    "\n",
    "PitsUnified_Finished=PitsUnified_Finished.reset_index() #reset index of added features, if this is not done, there is an error later\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationship between features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Relationship between all the variables\n",
    "\n",
    "#Calculating correlation: Heatmap\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"magma\",font_scale=0.7,font=\"serif\")\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "cmap = sns.diverging_palette(0, 210, 95, 49, as_cmap=True)\n",
    "sns.heatmap(PitsUnified_Finished.corr(), annot=True, fmt=\".2f\", \n",
    "           linewidths=5,cmap=cmap, vmin=-1, vmax=1, \n",
    "           cbar_kws={\"shrink\": .8}, square=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before making predictions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score #measures used to evaluate the models\n",
    "from sklearn.metrics import confusion_matrix, classification_report #confusion matrix to evaluate results\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV #Hyperparameter optimization\n",
    "from sklearn.model_selection import KFold #set kfold configuration\n",
    "from sklearn.model_selection import cross_val_score #cross validation\n",
    "from sklearn.metrics import make_scorer #set scores desired to train models\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#Set scorers\n",
    "f1_scorer=make_scorer(f1_score)\n",
    "accuracy_scorer=make_scorer(accuracy_score)\n",
    "\n",
    "### Function to test and evaluate the algorithms\n",
    "def testing_the_classifier(ticks_, thesize=(5,3)):\n",
    "    Train=pd.DataFrame()\n",
    "    Train[\"Predicted\"]=y_train_predicted\n",
    "    Train[\"Real\"]=y_train.tolist()\n",
    "\n",
    "    Test=pd.DataFrame()\n",
    "    Test[\"Predicted\"]=y_test_predicted\n",
    "    Test[\"Real\"]=y_test.tolist()\n",
    "\n",
    "    #Generate the confusion matrixes\n",
    "\n",
    "    cf_matrixtrain = confusion_matrix(Train[\"Real\"], Train[\"Predicted\"])\n",
    "    cf_matrixtest = confusion_matrix(Test[\"Real\"], Test[\"Predicted\"])\n",
    "\n",
    "    print(\"\\n Training Data:\")\n",
    "    sns.set_theme(style=\"whitegrid\", palette=\"BuPu\",font_scale=1,font=\"serif\")\n",
    "    plt.figure(figsize=thesize)\n",
    "    ax = sns.heatmap(cf_matrixtrain, annot=True,cmap=\"BuPu\",fmt='d')\n",
    "    ax.set_xlabel('\\nPredicted Values')\n",
    "    ax.set_ylabel('Actual Values ')\n",
    "    ax.xaxis.set_ticklabels(ticks_)\n",
    "    ax.yaxis.set_ticklabels(ticks_)\n",
    "    plt.show()\n",
    "\n",
    "    print(classification_report(y_train, y_train_predicted))\n",
    "    \n",
    "\n",
    "    print(\"\\n Testing Data:\")\n",
    "    sns.set_theme(style=\"whitegrid\", palette=\"BuPu\",font_scale=1,font=\"serif\")\n",
    "    plt.figure(figsize=thesize)\n",
    "    ax = sns.heatmap(cf_matrixtest, annot=True,cmap=\"BuPu\",fmt='d')\n",
    "    ax.set_xlabel('\\nPredicted Values')\n",
    "    ax.set_ylabel('Actual Values ')\n",
    "    ax.xaxis.set_ticklabels(ticks_)\n",
    "    ax.yaxis.set_ticklabels(ticks_)\n",
    "    plt.show()\n",
    "\n",
    "    print(classification_report(y_test, y_test_predicted))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bar plot: class distribution\n",
    "g=sns.catplot(x=\"Num_Pits\",data=PitsUnified_Finished,kind=\"count\",height=5,aspect=2)\n",
    "g.set(xlabel=\"Number of pits in our data\")\n",
    "plt.show()\n",
    "\n",
    "#There exists a class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(PitsUnified_Finished[PitsUnified_Finished[\"Num_Pits\"]==3]) #number of drivers-races with 3 pitstops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1st: Random Forest Classifier to predict the feature \"Num_Pits\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PitsUnified_Finished.columns\n",
    "\n",
    "Selected=['season','round','laps', 'driverId_enc',\n",
    "       'Circuit.circuitId_enc', 'Circuit.Location.country_enc',\n",
    "       'Constructor.constructorId_enc']\n",
    "\n",
    "selectednumber=31994 #randomseed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide data into training and testing - stratified\n",
    "from sklearn.model_selection import train_test_split #separte train and test data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(PitsUnified_Finished[Selected], PitsUnified_Finished[\"Num_Pits\"], test_size=0.20,stratify=PitsUnified_Finished[\"Num_Pits\"],random_state=selectednumber)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Cross Validation using Grid Search\n",
    "RF=RandomForestClassifier(random_state=selectednumber,n_estimators=100)\n",
    "tuned_parameters = [{'criterion': [\"gini\",\"entropy\"],\"max_features\":[\"auto\",\"sqrt\",\"log2\",None]}]\n",
    "# for x in range(2,10):\n",
    "#     # clf = GridSearchCV(RF, tuned_parameters, cv=KFold(n_splits=x), scoring=f1_scorer)\n",
    "#     # clf.fit(X_train, y_train)\n",
    "#     # print(\"Folds: \",x,\"- F1 score: \",clf.best_score_,\" \",clf.best_params_)\n",
    "#     #in this case the f1 scorer was nan in every fold\n",
    "#     clf2 = GridSearchCV(RF, tuned_parameters, cv=KFold(n_splits=x), scoring=accuracy_scorer)\n",
    "#     clf2.fit(X_train, y_train)\n",
    "#     print(\"Folds: \",x,\"- Accuracy: \",clf2.best_score_,\" \",clf2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF=RandomForestClassifier(n_estimators=100,random_state=selectednumber,criterion=\"gini\",max_features=\"auto\")\n",
    "RF.fit(X_train, y_train)\n",
    "y_train_predicted=RF.predict(X_train)\n",
    "y_test_predicted=RF.predict(X_test)\n",
    "np.unique(y_test_predicted)\n",
    "mse_train = mean_squared_error(y_train,y_train_predicted)\n",
    "mse_test = mean_squared_error(y_test,y_test_predicted)\n",
    "\n",
    "print(mse_train,\"-\",mse_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1st: Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_the_classifier([\"1\",\"2\",\"3\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2nd: Random Forest Classifier to predict the feature Lap of pit stops (if any)\n",
    "\n",
    "First, predicting the lap in which the first pit stop is done\n",
    "\n",
    "Secondly, predicting the lap in which the second pit stop is done (0 is also an option with 0 pit stops)\n",
    "\n",
    "Thirdly, predicting the lap in which the third pit stop is done (0 is also an option with 0 pit stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to evaluate the models\n",
    "from sklearn import metrics\n",
    "\n",
    "def evaluatelapspitstops(y_train,y_train_predicted,y_test,y_test_predicted):\n",
    "    y_true = y_train \n",
    "    y_pred = y_train_predicted \n",
    "\n",
    "    print(\"\\n Training Scores:\")\n",
    "    print('Mean Absolute Error (MAE):', metrics.mean_absolute_error(y_true, y_pred))\n",
    "    print('Mean Squared Error (MSE):', metrics.mean_squared_error(y_true, y_pred))\n",
    "    print('Root Mean Squared Error (RMSE):', metrics.mean_squared_error(y_true, y_pred, squared=False))\n",
    "    print('Mean Absolute Percentage Error (MAPE):', metrics.mean_absolute_percentage_error(y_true, y_pred))\n",
    "    print('Explained Variance Score:', metrics.explained_variance_score(y_true, y_pred))\n",
    "    print('Max Error:', metrics.max_error(y_true, y_pred))\n",
    "    print('Mean Squared Log Error:', metrics.mean_squared_log_error(y_true, y_pred))\n",
    "    print('Median Absolute Error:', metrics.median_absolute_error(y_true, y_pred))\n",
    "    print('R^2:', metrics.r2_score(y_true, y_pred))\n",
    "\n",
    "    y_true = y_test \n",
    "    y_pred = y_test_predicted \n",
    "    \n",
    "    print(\"\\n Testing Scores:\")\n",
    "    print('Mean Absolute Error (MAE):', metrics.mean_absolute_error(y_true, y_pred))\n",
    "    print('Mean Squared Error (MSE):', metrics.mean_squared_error(y_true, y_pred))\n",
    "    print('Root Mean Squared Error (RMSE):', metrics.mean_squared_error(y_true, y_pred, squared=False))\n",
    "    print('Mean Absolute Percentage Error (MAPE):', metrics.mean_absolute_percentage_error(y_true, y_pred))\n",
    "    print('Explained Variance Score:', metrics.explained_variance_score(y_true, y_pred))\n",
    "    print('Max Error:', metrics.max_error(y_true, y_pred))\n",
    "    print('Mean Squared Log Error:', metrics.mean_squared_log_error(y_true, y_pred))\n",
    "    print('Median Absolute Error:', metrics.median_absolute_error(y_true, y_pred))\n",
    "    print('R^2:', metrics.r2_score(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PitsUnified_Finished.columns\n",
    "\n",
    "Selected=['season','round','laps', 'driverId_enc',\n",
    "       'Circuit.circuitId_enc', 'Circuit.Location.country_enc',\n",
    "       'Constructor.constructorId_enc','Pit1_lap','Pit2_lap','Pit3_lap','Pit4_lap',\"Num_Pits\"]\n",
    "\n",
    "#The 3 features including the Pit laps will be taken out later\n",
    "\n",
    "#randomseed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PitsUnified_Finished.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide data into training and testing - stratified\n",
    "from sklearn.model_selection import train_test_split #separte train and test data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(PitsUnified_Finished[Selected], PitsUnified_Finished['Pit1_lap'], test_size=0.20,stratify=PitsUnified_Finished[\"Num_Pits\"],random_state=selectednumber)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(PitsUnified_Finished[Selected], PitsUnified_Finished['Pit1_lap'], test_size=0.20,random_state=selectednumber)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PitInfotrain=X_train[['Pit1_lap','Pit2_lap','Pit3_lap','Pit4_lap',\"Num_Pits\"]] #we needed them to have this info in the correct order for later\n",
    "X_train=X_train.drop(columns=['Pit1_lap','Pit2_lap','Pit3_lap','Pit4_lap',\"Num_Pits\"]) #the 3 features are dropped from the training set\n",
    "\n",
    "PitInfotest=X_test[['Pit1_lap','Pit2_lap','Pit3_lap','Pit4_lap',\"Num_Pits\"]] #we needed them to have this info in the correct order for later\n",
    "X_test=X_test.drop(columns=['Pit1_lap','Pit2_lap','Pit3_lap','Pit4_lap',\"Num_Pits\"]) #the 3 features are dropped from the training set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Cross Validation using Grid Search\n",
    "RF=RandomForestClassifier(random_state=selectednumber,n_estimators=100)\n",
    "tuned_parameters = [{'criterion': [\"gini\",\"entropy\"],\"max_features\":[\"auto\",\"sqrt\",\"log2\",None]}]\n",
    "# for x in range(2,10):\n",
    "#     # clf = GridSearchCV(RF, tuned_parameters, cv=KFold(n_splits=x), scoring=f1_scorer)\n",
    "#     # clf.fit(X_train, y_train)\n",
    "#     # print(\"Folds: \",x,\"- F1 score: \",clf.best_score_,\" \",clf.best_params_)\n",
    "#     #in this case the f1 scorer was nan in every fold\n",
    "#     clf2 = GridSearchCV(RF, tuned_parameters, cv=KFold(n_splits=x), scoring=accuracy_scorer)\n",
    "#     clf2.fit(X_train, y_train)\n",
    "#     print(\"Folds: \",x,\"- Accuracy: \",clf2.best_score_,\" \",clf2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF=RandomForestClassifier(n_estimators=100,random_state=selectednumber,criterion=\"gini\",max_features=\"auto\")\n",
    "RF.fit(X_train, y_train)\n",
    "y_train_predicted=RF.predict(X_train)\n",
    "y_test_predicted=RF.predict(X_test)\n",
    "\n",
    "ticks=PitsUnified_Finished[\"Pit1_lap\"].unique()\n",
    "\n",
    "evaluatelapspitstops(y_train,y_train_predicted,y_test,y_test_predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Including the info of the laps PREDICTED for the fist pitstop into the independent features in the training and testing set\n",
    "X_train[\"Pit1_lap_predicted\"]=y_train_predicted\n",
    "X_test[\"Pit1_lap_predicted\"]=y_test_predicted\n",
    "\n",
    "y_train=PitInfotrain['Pit2_lap'].copy(deep=True) #changing the dependent feauture to pitstops2\n",
    "y_test=PitInfotest['Pit2_lap'].copy(deep=True) #changing the dependent feauture to pitstops2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF=RandomForestClassifier(n_estimators=100,random_state=selectednumber,criterion=\"gini\",max_features=\"auto\")\n",
    "RF.fit(X_train, y_train)\n",
    "y_train_predicted=RF.predict(X_train)\n",
    "y_test_predicted=RF.predict(X_test)\n",
    "\n",
    "ticks=PitsUnified_Finished[\"Pit2_lap\"].unique()\n",
    "\n",
    "evaluatelapspitstops(y_train,y_train_predicted,y_test,y_test_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Including the info of the laps PREDICTED for the second pitstop into the independent features in the training and testing set\n",
    "X_train[\"Pit2_lap_predicted\"]=y_train_predicted\n",
    "X_test[\"Pit2_lap_predicted\"]=y_test_predicted\n",
    "\n",
    "y_train=PitInfotrain['Pit3_lap'].copy(deep=True) #changing the dependent feauture to pitstops3\n",
    "y_test=PitInfotest['Pit3_lap'].copy(deep=True) #changing the dependent feauture to pitstops3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF=RandomForestClassifier(n_estimators=100,random_state=selectednumber,criterion=\"gini\",max_features=\"auto\")\n",
    "RF.fit(X_train, y_train)\n",
    "y_train_predicted=RF.predict(X_train)\n",
    "y_test_predicted=RF.predict(X_test)\n",
    "\n",
    "ticks=PitsUnified_Finished[\"Pit3_lap\"].unique()\n",
    "\n",
    "evaluatelapspitstops(y_train,y_train_predicted,y_test,y_test_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Including pits4 information\n",
    "X_train[\"Pit4_lap\"]=PitInfotrain['Pit4_lap'] \n",
    "X_test[\"Pit4_lap\"]=PitInfotest['Pit4_lap'] \n",
    "\n",
    "#Including the info of the laps PREDICTED for the second pitstop into the independent features in the training and testing set\n",
    "X_train[\"Pit3_lap_predicted\"]=y_train_predicted\n",
    "X_test[\"Pit3_lap_predicted\"]=y_test_predicted\n",
    "\n",
    "#Adding the total number of pits per driver and per race p1\n",
    "conditionsX_train = [\n",
    "    (X_train['Pit4_lap'] > 0),\n",
    "    (X_train['Pit4_lap'] == 0) & (X_train['Pit3_lap_predicted'] > 0),\n",
    "    (X_train['Pit4_lap'] == 0) & (X_train['Pit3_lap_predicted'] == 0) & (X_train['Pit2_lap_predicted'] > 0),\n",
    "    (X_train['Pit4_lap'] == 0) & (X_train['Pit3_lap_predicted'] == 0) & (X_train['Pit2_lap_predicted'] == 0) & (X_train['Pit1_lap_predicted'] > 0)\n",
    "]\n",
    "\n",
    "conditionsX_test = [\n",
    "    (X_test['Pit4_lap'] > 0),\n",
    "    (X_test['Pit4_lap'] == 0) & (X_test['Pit3_lap_predicted'] > 0),\n",
    "    (X_test['Pit4_lap'] == 0) & (X_test['Pit3_lap_predicted'] == 0) & (X_test['Pit2_lap_predicted'] > 0),\n",
    "    (X_test['Pit4_lap'] == 0) & (X_test['Pit3_lap_predicted'] == 0) & (X_test['Pit2_lap_predicted'] == 0) & (X_test['Pit1_lap_predicted'] > 0)\n",
    "]\n",
    "\n",
    "values = [4, 3, 2, 1]\n",
    "\n",
    "#Adding the total number of pits per driver and per race p2\n",
    "X_train['Num_Pits_Predicted'] = np.select(conditionsX_train, values)\n",
    "X_test['Num_Pits_Predicted'] = np.select(conditionsX_test, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=PitInfotrain['Num_Pits']\n",
    "y_test=PitInfotest['Num_Pits']\n",
    "\n",
    "y_train_predicted=X_train['Num_Pits_Predicted']\n",
    "y_test_predicted=X_test['Num_Pits_Predicted']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2nd: Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_the_classifier([\"1\",\"2\",\"3\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN1"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "250b308242d36e83629721c7244075b381b1b678cf3b391cdc434a91845cedac"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
